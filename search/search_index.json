{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"thebook Hints, reads and tips for me.","title":"Introduction"},{"location":"#thebook","text":"Hints, reads and tips for me.","title":"thebook"},{"location":"architectural_pattern/","text":"","title":"Architectural pattern"},{"location":"architectural_pattern/event_sourcing/","text":"Further reads https://ookami86.github.io/event-sourcing-in-practice/ https://martinfowler.com/eaaDev/EventSourcing.html https://docs.microsoft.com/en-us/azure/architecture/patterns/event-sourcing","title":"Event sourcing"},{"location":"architectural_pattern/event_sourcing/#further-reads","text":"https://ookami86.github.io/event-sourcing-in-practice/ https://martinfowler.com/eaaDev/EventSourcing.html https://docs.microsoft.com/en-us/azure/architecture/patterns/event-sourcing","title":"Further reads"},{"location":"architectural_pattern/feature_flags/","text":"Further reads DevOps at MS: Progressive experimentation MS: Use feature Flags (dotnet core) YT: How to use feature flags in .Net Core","title":"Feature flags"},{"location":"architectural_pattern/feature_flags/#further-reads","text":"DevOps at MS: Progressive experimentation MS: Use feature Flags (dotnet core) YT: How to use feature flags in .Net Core","title":"Further reads"},{"location":"architectural_pattern/microservices/","text":"Documentation How to: document microservices Styles Independent Systems Architecture (ISA) Self-contained System (SCS) Further reads vs. Monoliths https://martinfowler.com/bliki/MicroservicePremium.html https://docs.microsoft.com/en-us/azure/architecture/microservices/ https://microservices.io/ https://servicemesh.es/","title":"Microservices"},{"location":"architectural_pattern/microservices/#documentation","text":"How to: document microservices","title":"Documentation"},{"location":"architectural_pattern/microservices/#styles","text":"Independent Systems Architecture (ISA) Self-contained System (SCS)","title":"Styles"},{"location":"architectural_pattern/microservices/#further-reads","text":"vs. Monoliths https://martinfowler.com/bliki/MicroservicePremium.html https://docs.microsoft.com/en-us/azure/architecture/microservices/ https://microservices.io/ https://servicemesh.es/","title":"Further reads"},{"location":"architectural_pattern/sagas_pattern/","text":"Reference implementation https://github.com/aserowy/TreeLine.Sagas Further reads https://microservices.io/patterns/data/saga.html https://docs.microsoft.com/en-us/azure/architecture/patterns/choreography","title":"Sagas pattern"},{"location":"architectural_pattern/sagas_pattern/#reference-implementation","text":"https://github.com/aserowy/TreeLine.Sagas","title":"Reference implementation"},{"location":"architectural_pattern/sagas_pattern/#further-reads","text":"https://microservices.io/patterns/data/saga.html https://docs.microsoft.com/en-us/azure/architecture/patterns/choreography","title":"Further reads"},{"location":"concepts/document_microservices/","text":"Motivation Documentation is usually a burden for developers and is often neglected. Especially in microservices, documentation is essential to understand how the service landscape is structured and where which capabilities lie. In addition, it should be possible to answer core questions, such as overarching concepts, architecture decisions and also goals, simply and sustainably. It is important to use a documentation template that requires minimal maintenance to ensure a high degree of up-to-dateness and to reduce inhibitions of developers to maintain documentation. Context A minimalistic approach This approach is structured into four documentation types to describe the macro and micro architectures of a project. Method and example were described by Stefan Z\u00f6rner . At first the minimal architecture overview provides insights about the mission and context of the architecture. Furthermore, solution approaches, specifications, non-functional requirements, and objectives of the architecture are also defined. Through context delimitation and mission objectives, concrete decisions for the macro architecture can be made and distributed through concepts within the team. Concepts describe why, what and how these decisions should be implemented and provide the framework for making decisions for the micro architecture within the individual services. In order to also document the micro architecture of the microservices, a profile is created for each service, in which specific information such as contact persons, approaches to solutions, but also technical debts are recorded. Micro vs. macro architecture One of the Independent Systems Architecture (ISA) principles states, that each... \"[...] system must have two clearly separated levels of architectural decisions. The macro architecture comprises decisions that cover all modules. [...] The micro architecture considers decisions which may be taken individually for each module\". This leads to a fundamental decision whether a solution option at the macro-architecture level should lead to standardisation for all services or whether the solution can be re-evaluated in each service to ensure the best possible outcome. In other words, decisions at macro level offer standardisation, whereas decisions at micro level create individualisation. Minimal architecture overview Structure The overview consists of three parts. In the first part, the task, the mission objective and context boundaries are used to describe which objectives are to be achieved. In the second part, influences define both external specifications and architectural goals. These goals are recorded in the form of non-functional requirements and are derived from the work result of the first part. Based on the defined influences, the third part contains concrete solution approaches. For example, architectural styles or concrete technologies can be defined to solve certain challenges. Task In the mission statement, the tasks and objectives of the system are described in bullet points or short sentences. The following guiding questions should be answered briefly and concisely: What is the software system for? What is the central sales/use argument? (\"claim\", \"slogan\") Who benefits from it? What are the main features of the system? How does it differ from products of competitors, or the previous version? By means of context boundaries, the mission statement is narrowed down and users and external systems with which the system interacts are visualised. Influences Influences describe core requirements of the system by means of two vectors. On the one hand, external specifications reduce the solution space and exclude options in advance. These include technical requirements such as \"we are going to the cloud\" as well as organisational requirements such as working methods or team composition. On the other hand, the top three to five non-functional requirements are defined and tabulated by means of a short description or motivation in order to describe the architectural goals of the system. It is crucial to take these into account as early as possible in the decision-making process, as these requirements shape the solution and can only be integrated subsequently at great additional expense. It is also important that each requirement of the system is checked for interactions between the individual influences, as influences can also exclude each other. For example, it would make sense to introduce CAPTCHAs at login to increase security. However, usability would suffer because users find additional security measures disturbing. In such cases, compromises are made. Solution approaches The solutions are derived from the previously defined influences and assigned to the non-functional requirements. In this way, a table is developed that specifies concrete \"high level\" solutions for each requirement. As an example, the requirement availability can be assigned to \"operation in the Azure Cloud\" and \"use of the circuit breaker pattern\" in order to increase availability and resilience. It is important to note that the defined solutions can be pure ideas or approaches as well as concrete decisions or concepts (see below). To concretise these solutions, the approaches are visualised in a diagram of the architecture. The aim here is not to provide a complete overview, but to illustrate where the solutions are used. This is important to achieve a delineation of which requirements need to be considered at which levels of the architecture. For example, if resilience is covered by a service mesh in k8s, the circuit breaker pattern would no longer need to be integrated into the individual services. Decisions Decisions are an essential part of the architecture documentation. Decisions define concrete specifications in order to drive standardised solutions into the architecture. This does not mean that decisions are imposed from above, but should be discussed, developed and made jointly by the team. In order to make architectural decisions comprehensible and thus achieve greater acceptance in the team, the following five phases of decision-making are documented: In the template for architectural decisions , the phases are enriched with additional questions. The template thus provides a basis for what content should be documented in the respective phases. But how exactly does a question arise in the first place? Basically, the need for decisions arises from mission statement, architectural goals and the context boundaries, to document for example solution approaches or the introduction of concepts. In this context, typical questions would be: What user interfaces do the different users get? How do we integrate certain external systems? Which product/technology do we use to solve certain requirements? How do components of our system communicate with each other? How do we address cross-cutting issues such as logging or resilience? Make or buy? A good starting point for questions would be the centralisation of the UI, centralisation of security and whether or how internal services communicate with each other. Of course, these topics should only be the beginning to describe the architecture on the basis of decisions! Concepts Concepts are the last building block of macro architecture. They make it possible to document the implementation of decisions made and answer why what is solved and how. A standardised structure allows concepts to be managed in a project-neutral way, for example in a team or company, and to be integrated into one's own architecture documentation if required. This reduces the amount of documentation that has to be written and increases the flexibility of the team composition through the standardisation of the solutions. Profiles Typical contents for a service profile: Responsibility i.e. what does the service do Special requirements Technology Stack Interface definition (API) Important ideas / solutions for the service contact person Plans for the future Technical debts Example Next steps ADRs","title":"How to document microservices"},{"location":"concepts/document_microservices/#motivation","text":"Documentation is usually a burden for developers and is often neglected. Especially in microservices, documentation is essential to understand how the service landscape is structured and where which capabilities lie. In addition, it should be possible to answer core questions, such as overarching concepts, architecture decisions and also goals, simply and sustainably. It is important to use a documentation template that requires minimal maintenance to ensure a high degree of up-to-dateness and to reduce inhibitions of developers to maintain documentation.","title":"Motivation"},{"location":"concepts/document_microservices/#context","text":"","title":"Context"},{"location":"concepts/document_microservices/#a-minimalistic-approach","text":"This approach is structured into four documentation types to describe the macro and micro architectures of a project. Method and example were described by Stefan Z\u00f6rner . At first the minimal architecture overview provides insights about the mission and context of the architecture. Furthermore, solution approaches, specifications, non-functional requirements, and objectives of the architecture are also defined. Through context delimitation and mission objectives, concrete decisions for the macro architecture can be made and distributed through concepts within the team. Concepts describe why, what and how these decisions should be implemented and provide the framework for making decisions for the micro architecture within the individual services. In order to also document the micro architecture of the microservices, a profile is created for each service, in which specific information such as contact persons, approaches to solutions, but also technical debts are recorded.","title":"A minimalistic approach"},{"location":"concepts/document_microservices/#micro-vs-macro-architecture","text":"One of the Independent Systems Architecture (ISA) principles states, that each... \"[...] system must have two clearly separated levels of architectural decisions. The macro architecture comprises decisions that cover all modules. [...] The micro architecture considers decisions which may be taken individually for each module\". This leads to a fundamental decision whether a solution option at the macro-architecture level should lead to standardisation for all services or whether the solution can be re-evaluated in each service to ensure the best possible outcome. In other words, decisions at macro level offer standardisation, whereas decisions at micro level create individualisation.","title":"Micro vs. macro architecture"},{"location":"concepts/document_microservices/#minimal-architecture-overview","text":"","title":"Minimal architecture overview"},{"location":"concepts/document_microservices/#structure","text":"The overview consists of three parts. In the first part, the task, the mission objective and context boundaries are used to describe which objectives are to be achieved. In the second part, influences define both external specifications and architectural goals. These goals are recorded in the form of non-functional requirements and are derived from the work result of the first part. Based on the defined influences, the third part contains concrete solution approaches. For example, architectural styles or concrete technologies can be defined to solve certain challenges.","title":"Structure"},{"location":"concepts/document_microservices/#task","text":"In the mission statement, the tasks and objectives of the system are described in bullet points or short sentences. The following guiding questions should be answered briefly and concisely: What is the software system for? What is the central sales/use argument? (\"claim\", \"slogan\") Who benefits from it? What are the main features of the system? How does it differ from products of competitors, or the previous version? By means of context boundaries, the mission statement is narrowed down and users and external systems with which the system interacts are visualised.","title":"Task"},{"location":"concepts/document_microservices/#influences","text":"Influences describe core requirements of the system by means of two vectors. On the one hand, external specifications reduce the solution space and exclude options in advance. These include technical requirements such as \"we are going to the cloud\" as well as organisational requirements such as working methods or team composition. On the other hand, the top three to five non-functional requirements are defined and tabulated by means of a short description or motivation in order to describe the architectural goals of the system. It is crucial to take these into account as early as possible in the decision-making process, as these requirements shape the solution and can only be integrated subsequently at great additional expense. It is also important that each requirement of the system is checked for interactions between the individual influences, as influences can also exclude each other. For example, it would make sense to introduce CAPTCHAs at login to increase security. However, usability would suffer because users find additional security measures disturbing. In such cases, compromises are made.","title":"Influences"},{"location":"concepts/document_microservices/#solution-approaches","text":"The solutions are derived from the previously defined influences and assigned to the non-functional requirements. In this way, a table is developed that specifies concrete \"high level\" solutions for each requirement. As an example, the requirement availability can be assigned to \"operation in the Azure Cloud\" and \"use of the circuit breaker pattern\" in order to increase availability and resilience. It is important to note that the defined solutions can be pure ideas or approaches as well as concrete decisions or concepts (see below). To concretise these solutions, the approaches are visualised in a diagram of the architecture. The aim here is not to provide a complete overview, but to illustrate where the solutions are used. This is important to achieve a delineation of which requirements need to be considered at which levels of the architecture. For example, if resilience is covered by a service mesh in k8s, the circuit breaker pattern would no longer need to be integrated into the individual services.","title":"Solution approaches"},{"location":"concepts/document_microservices/#decisions","text":"Decisions are an essential part of the architecture documentation. Decisions define concrete specifications in order to drive standardised solutions into the architecture. This does not mean that decisions are imposed from above, but should be discussed, developed and made jointly by the team. In order to make architectural decisions comprehensible and thus achieve greater acceptance in the team, the following five phases of decision-making are documented: In the template for architectural decisions , the phases are enriched with additional questions. The template thus provides a basis for what content should be documented in the respective phases. But how exactly does a question arise in the first place? Basically, the need for decisions arises from mission statement, architectural goals and the context boundaries, to document for example solution approaches or the introduction of concepts. In this context, typical questions would be: What user interfaces do the different users get? How do we integrate certain external systems? Which product/technology do we use to solve certain requirements? How do components of our system communicate with each other? How do we address cross-cutting issues such as logging or resilience? Make or buy? A good starting point for questions would be the centralisation of the UI, centralisation of security and whether or how internal services communicate with each other. Of course, these topics should only be the beginning to describe the architecture on the basis of decisions!","title":"Decisions"},{"location":"concepts/document_microservices/#concepts","text":"Concepts are the last building block of macro architecture. They make it possible to document the implementation of decisions made and answer why what is solved and how. A standardised structure allows concepts to be managed in a project-neutral way, for example in a team or company, and to be integrated into one's own architecture documentation if required. This reduces the amount of documentation that has to be written and increases the flexibility of the team composition through the standardisation of the solutions.","title":"Concepts"},{"location":"concepts/document_microservices/#profiles","text":"Typical contents for a service profile: Responsibility i.e. what does the service do Special requirements Technology Stack Interface definition (API) Important ideas / solutions for the service contact person Plans for the future Technical debts","title":"Profiles"},{"location":"concepts/document_microservices/#example","text":"","title":"Example"},{"location":"concepts/document_microservices/#next-steps","text":"ADRs","title":"Next steps"},{"location":"concepts/git_feature_trunk/","text":"Motivation In smaller teams big git strategies like git flow are too heavy. On the other hand, a normal trunk based approach does need technics like feature flags and high test coverage to ensure high quality deployments. In order to use a light weight strategy with a good quality regarding deployments we take another approach: Feature, trunk, and release branches. Context Day to day work For small changes we operate directly on main/master. This is our trunk. The trunk gets continously deployed on an integration environment on each push. Larger changes get developed in a feature branch. All feature branches are branched from the trunk and grouped in a feature folder. You can achieve these folders through naming (e.g. features/branch-name while branching). For larger fixes the same applies. Only the naming differs slightly: fixes/branch-name In both cases merges from trunk into feature or fixe branches should happen at least every week once. Releases Each major release gets its own release branch. The naming is equivalent to features and fixes: releases/version-name After the release got deployed and fixes arose these will get cherry picked from the trunk into the current release branch. Releases will never get merged back into trunk! The trunk will never get merged into release branches!","title":"Feature trunk git strategy"},{"location":"concepts/git_feature_trunk/#motivation","text":"In smaller teams big git strategies like git flow are too heavy. On the other hand, a normal trunk based approach does need technics like feature flags and high test coverage to ensure high quality deployments. In order to use a light weight strategy with a good quality regarding deployments we take another approach: Feature, trunk, and release branches.","title":"Motivation"},{"location":"concepts/git_feature_trunk/#context","text":"","title":"Context"},{"location":"concepts/git_feature_trunk/#day-to-day-work","text":"For small changes we operate directly on main/master. This is our trunk. The trunk gets continously deployed on an integration environment on each push. Larger changes get developed in a feature branch. All feature branches are branched from the trunk and grouped in a feature folder. You can achieve these folders through naming (e.g. features/branch-name while branching). For larger fixes the same applies. Only the naming differs slightly: fixes/branch-name In both cases merges from trunk into feature or fixe branches should happen at least every week once.","title":"Day to day work"},{"location":"concepts/git_feature_trunk/#releases","text":"Each major release gets its own release branch. The naming is equivalent to features and fixes: releases/version-name After the release got deployed and fixes arose these will get cherry picked from the trunk into the current release branch. Releases will never get merged back into trunk! The trunk will never get merged into release branches!","title":"Releases"},{"location":"concepts/test_rules_in_csharp/","text":"Motivation To test rules, it is useful to make test data of specific rules available to all other rules. This way not only positive but also negative tests can be performed for all rules. Context In order to ensure that for individual rules the results for the test date and the respective rule are also run through when the test data is extended, the generation of test data and test results must be separated. Thus, leading to a resolver for test data only. The resolver returns a dictionary of test data type and data, to examine which test data resulted in e.g. failed tests. To ensure reusability for tests the enumeration is implemented as an abstract class, which yields each key value pair and checks if the pair is specified in our test or fails otherwise. In the test class a test data collection gets implemented and inherets from the given abstract class. It holds the relationship between test data type and expected result as an array of objects. With Theory and ClassData(typeof()) each yield can now be tested against the given test case. Step by step At first we create the data resolver to generate test data with a test data type. internal sealed class AnalyzerMockTestDataResolver { public IDictionary < AnalyzerMockTestDataType , IList < ISagaProfileAnalyzer >> Get () { return new Dictionary < AnalyzerMockTestDataType , IList < ISagaProfileAnalyzer >> { { AnalyzerMockTestDataType . NoSagasRegistered , GetNoSagasRegistered ()}, { AnalyzerMockTestDataType . ValidAnalyzers , GetValidAnalyzers ()}, { AnalyzerMockTestDataType . EqualAnalyzerIdentifier , GetEqualAnalyzerIdentifier ()}, { AnalyzerMockTestDataType . EqualVersionIdentifier , GetEqualVersionIdentifier ()}, { AnalyzerMockTestDataType . VersionWithoutSteps , GetVersionWithoutSteps ()} }; } private static IList < ISagaProfileAnalyzer > GetNoSagasRegistered () { return new List < ISagaProfileAnalyzer >(); } private static IList < ISagaProfileAnalyzer > GetValidAnalyzers () { var result = new List < ISagaProfileAnalyzer >(); var analyzer01 = new SagaProfileAnalyzerMock ( \"1\" ); result . Add ( analyzer01 ); analyzer01 . AddVersion ( \"1.1.0\" ) . AddStep < SagaEvent01 , SagaStep01Mock >() . AddStep < SagaEvent02 , SagaStep02Mock >(); ... In order to ensure that all test data is passed through, we provide an abstract provider. This is used to deliver test data and test specific results. [SuppressMessage( \"Naming\", \"CA1710:Identifiers should have correct suffix\", Justification = \"abstract class name should end with base\")] public abstract class AnalyzerMockTestDataCollectionBase : IEnumerable < object []> { internal abstract object []? GetReturnValuesByDataType ( AnalyzerMockTestDataType type ); public IEnumerator < object []> GetEnumerator () { var resolver = new AnalyzerMockTestDataResolver (); foreach ( var kvp in resolver . Get ()) { var values = GetReturnValuesByDataType ( kvp . Key ); if ( values is null ) { throw new KeyNotFoundException ( $ \"{GetType().Name} does not contain \" + $ \"return values for test data of type \" + $ \"{Enum.GetName(typeof(AnalyzerMockTestDataType), kvp.Key)}.\" ); } var result = new List < object > { kvp . Value }; result . AddRange ( values ); yield return result . ToArray (); } } IEnumerator IEnumerable . GetEnumerator () { return GetEnumerator (); } } Now we can use the base class and include it in our tests. public class MultipleVersionsWithEqualIdentifierRuleTests { private MultipleVersionsWithEqualIdentifierRule CreateMultipleVersionsWithEqualIdentifierRule () { return new MultipleVersionsWithEqualIdentifierRule (); } private class AnalyzerMockTestDataCollection : AnalyzerMockTestDataCollectionBase { internal override object []? GetReturnValuesByDataType ( AnalyzerMockTestDataType type ) { return type switch { AnalyzerMockTestDataType . NoSagasRegistered => new object [] { 0 , 0 }, AnalyzerMockTestDataType . ValidAnalyzers => new object [] { 0 , 0 }, AnalyzerMockTestDataType . EqualAnalyzerIdentifier => new object [] { 0 , 0 }, AnalyzerMockTestDataType . EqualVersionIdentifier => new object [] { 0 , 2 }, AnalyzerMockTestDataType . VersionWithoutSteps => new object [] { 0 , 0 }, _ => null }; } } [Theory] [ClassData(typeof(AnalyzerMockTestDataCollection))] internal void Validate ( IList < ISagaProfileAnalyzer > analyzers , int countWarnings , int countExceptions ) { // Arrange var multipleVersionsWithEqualIdentifierRule = CreateMultipleVersionsWithEqualIdentifierRule (); // Act var ( warnings , exceptions ) = multipleVersionsWithEqualIdentifierRule . Validate ( analyzers ); // Assert Assert . Equal ( countWarnings , warnings ?. Count () ?? 0 ); Assert . Equal ( countExceptions , exceptions ?. Count () ?? 0 ); } } Next steps For a resuable structure you should use dependency injection to decouple test data generation and GetEnumerator() .","title":"How to test rules in C#"},{"location":"concepts/test_rules_in_csharp/#motivation","text":"To test rules, it is useful to make test data of specific rules available to all other rules. This way not only positive but also negative tests can be performed for all rules.","title":"Motivation"},{"location":"concepts/test_rules_in_csharp/#context","text":"In order to ensure that for individual rules the results for the test date and the respective rule are also run through when the test data is extended, the generation of test data and test results must be separated. Thus, leading to a resolver for test data only. The resolver returns a dictionary of test data type and data, to examine which test data resulted in e.g. failed tests. To ensure reusability for tests the enumeration is implemented as an abstract class, which yields each key value pair and checks if the pair is specified in our test or fails otherwise. In the test class a test data collection gets implemented and inherets from the given abstract class. It holds the relationship between test data type and expected result as an array of objects. With Theory and ClassData(typeof()) each yield can now be tested against the given test case.","title":"Context"},{"location":"concepts/test_rules_in_csharp/#step-by-step","text":"At first we create the data resolver to generate test data with a test data type. internal sealed class AnalyzerMockTestDataResolver { public IDictionary < AnalyzerMockTestDataType , IList < ISagaProfileAnalyzer >> Get () { return new Dictionary < AnalyzerMockTestDataType , IList < ISagaProfileAnalyzer >> { { AnalyzerMockTestDataType . NoSagasRegistered , GetNoSagasRegistered ()}, { AnalyzerMockTestDataType . ValidAnalyzers , GetValidAnalyzers ()}, { AnalyzerMockTestDataType . EqualAnalyzerIdentifier , GetEqualAnalyzerIdentifier ()}, { AnalyzerMockTestDataType . EqualVersionIdentifier , GetEqualVersionIdentifier ()}, { AnalyzerMockTestDataType . VersionWithoutSteps , GetVersionWithoutSteps ()} }; } private static IList < ISagaProfileAnalyzer > GetNoSagasRegistered () { return new List < ISagaProfileAnalyzer >(); } private static IList < ISagaProfileAnalyzer > GetValidAnalyzers () { var result = new List < ISagaProfileAnalyzer >(); var analyzer01 = new SagaProfileAnalyzerMock ( \"1\" ); result . Add ( analyzer01 ); analyzer01 . AddVersion ( \"1.1.0\" ) . AddStep < SagaEvent01 , SagaStep01Mock >() . AddStep < SagaEvent02 , SagaStep02Mock >(); ... In order to ensure that all test data is passed through, we provide an abstract provider. This is used to deliver test data and test specific results. [SuppressMessage( \"Naming\", \"CA1710:Identifiers should have correct suffix\", Justification = \"abstract class name should end with base\")] public abstract class AnalyzerMockTestDataCollectionBase : IEnumerable < object []> { internal abstract object []? GetReturnValuesByDataType ( AnalyzerMockTestDataType type ); public IEnumerator < object []> GetEnumerator () { var resolver = new AnalyzerMockTestDataResolver (); foreach ( var kvp in resolver . Get ()) { var values = GetReturnValuesByDataType ( kvp . Key ); if ( values is null ) { throw new KeyNotFoundException ( $ \"{GetType().Name} does not contain \" + $ \"return values for test data of type \" + $ \"{Enum.GetName(typeof(AnalyzerMockTestDataType), kvp.Key)}.\" ); } var result = new List < object > { kvp . Value }; result . AddRange ( values ); yield return result . ToArray (); } } IEnumerator IEnumerable . GetEnumerator () { return GetEnumerator (); } } Now we can use the base class and include it in our tests. public class MultipleVersionsWithEqualIdentifierRuleTests { private MultipleVersionsWithEqualIdentifierRule CreateMultipleVersionsWithEqualIdentifierRule () { return new MultipleVersionsWithEqualIdentifierRule (); } private class AnalyzerMockTestDataCollection : AnalyzerMockTestDataCollectionBase { internal override object []? GetReturnValuesByDataType ( AnalyzerMockTestDataType type ) { return type switch { AnalyzerMockTestDataType . NoSagasRegistered => new object [] { 0 , 0 }, AnalyzerMockTestDataType . ValidAnalyzers => new object [] { 0 , 0 }, AnalyzerMockTestDataType . EqualAnalyzerIdentifier => new object [] { 0 , 0 }, AnalyzerMockTestDataType . EqualVersionIdentifier => new object [] { 0 , 2 }, AnalyzerMockTestDataType . VersionWithoutSteps => new object [] { 0 , 0 }, _ => null }; } } [Theory] [ClassData(typeof(AnalyzerMockTestDataCollection))] internal void Validate ( IList < ISagaProfileAnalyzer > analyzers , int countWarnings , int countExceptions ) { // Arrange var multipleVersionsWithEqualIdentifierRule = CreateMultipleVersionsWithEqualIdentifierRule (); // Act var ( warnings , exceptions ) = multipleVersionsWithEqualIdentifierRule . Validate ( analyzers ); // Assert Assert . Equal ( countWarnings , warnings ?. Count () ?? 0 ); Assert . Equal ( countExceptions , exceptions ?. Count () ?? 0 ); } }","title":"Step by step"},{"location":"concepts/test_rules_in_csharp/#next-steps","text":"For a resuable structure you should use dependency injection to decouple test data generation and GetEnumerator() .","title":"Next steps"},{"location":"concepts/versioning_with_semver/","text":"Motivation Version numbers are used in practical terms by the consumer, or client, to identify or compare their copy of the software product against another copy, such as the newest version released by the developer. For the programmer or company, versioning is often used on a revision-by-revision basis, where individual parts of the software are compared and contrasted with newer or older revisions of those same parts, often in a collaborative version control system. ^1 It is crucial that errors can be directly assigned to a specific software version. In this way, fixes and features can also be assigned to software versions. This traceability increases the overview for customers and developers. To make releases easier to plan, it is important to make future versions predictable. Semantic versioning serves as an example here. Context Semantic versioning is a formal convention for specifying compatibility using a three-part version number: major version; minor version; and patch. The patch number is incremented for minor changes and bug fixes which do not change the software's application programming interface (API). The minor version is incremented for releases which add new, but backward-compatible, API features, and the major version is incremented for API changes which are not backward-compatible. For example, software which relies on version 2.1.5 of an API is compatible with version 2.2.3, but not necessarily with 3.2.4. ^1 Semantic Versioning states, that a version changes under specific triggers. In short ^2 : Given a version number MAJOR.MINOR.PATCH, increment the: MAJOR version when you make incompatible API changes, MINOR version when you add functionality in a backwards compatible manner, and PATCH version when you make backwards compatible bug fixes. These triggers are not feasible in a software solution that is used by customers, for example. So to use semantic versioning, we have to redefine triggers for specific version changes. In this type of software solution MAJOR changes when new feature sets are added features are revised and are not backward compatible MINOR changes when new features are added to existing feature sets features are revised and are backwards compatible PATCH changes when fixes are provided To ensure that these rules are used throughout the software lifecycle and are predictable for future releases, define these triggers as follows: Version Reasons MAJOR ... MINOR ... PATCH ... Step by step In a banking context, a software solution is used to plan and control funds. Every year, the planning year is changed to enable planning for the coming year. These changes are made programmatically, as new regulations must also be implemented. In the middle of the year, the same changes are also made for the controlling. In addition, new features and critical bug fixes are implemented and rolled out in parallel. For each rollout, several deployments are done in staging until all bugs are fixed and these features can be released. In this situation, we define our triggers in such a way that they map the turn of the year and thus roughly indicate to the user via the version which adjustments have been released: Version Reasons MAJOR planning year gets increased, controlling year gets increased MINOR new features get integrated PATCH bugfixing Extensions Semantic versioning is not the only way to design software versions. It remains important that version numbers remain predictable and that references to features and fixes can be made.","title":"How to version software with Semantic Versioning"},{"location":"concepts/versioning_with_semver/#motivation","text":"Version numbers are used in practical terms by the consumer, or client, to identify or compare their copy of the software product against another copy, such as the newest version released by the developer. For the programmer or company, versioning is often used on a revision-by-revision basis, where individual parts of the software are compared and contrasted with newer or older revisions of those same parts, often in a collaborative version control system. ^1 It is crucial that errors can be directly assigned to a specific software version. In this way, fixes and features can also be assigned to software versions. This traceability increases the overview for customers and developers. To make releases easier to plan, it is important to make future versions predictable. Semantic versioning serves as an example here.","title":"Motivation"},{"location":"concepts/versioning_with_semver/#context","text":"Semantic versioning is a formal convention for specifying compatibility using a three-part version number: major version; minor version; and patch. The patch number is incremented for minor changes and bug fixes which do not change the software's application programming interface (API). The minor version is incremented for releases which add new, but backward-compatible, API features, and the major version is incremented for API changes which are not backward-compatible. For example, software which relies on version 2.1.5 of an API is compatible with version 2.2.3, but not necessarily with 3.2.4. ^1 Semantic Versioning states, that a version changes under specific triggers. In short ^2 : Given a version number MAJOR.MINOR.PATCH, increment the: MAJOR version when you make incompatible API changes, MINOR version when you add functionality in a backwards compatible manner, and PATCH version when you make backwards compatible bug fixes. These triggers are not feasible in a software solution that is used by customers, for example. So to use semantic versioning, we have to redefine triggers for specific version changes. In this type of software solution MAJOR changes when new feature sets are added features are revised and are not backward compatible MINOR changes when new features are added to existing feature sets features are revised and are backwards compatible PATCH changes when fixes are provided To ensure that these rules are used throughout the software lifecycle and are predictable for future releases, define these triggers as follows: Version Reasons MAJOR ... MINOR ... PATCH ...","title":"Context"},{"location":"concepts/versioning_with_semver/#step-by-step","text":"In a banking context, a software solution is used to plan and control funds. Every year, the planning year is changed to enable planning for the coming year. These changes are made programmatically, as new regulations must also be implemented. In the middle of the year, the same changes are also made for the controlling. In addition, new features and critical bug fixes are implemented and rolled out in parallel. For each rollout, several deployments are done in staging until all bugs are fixed and these features can be released. In this situation, we define our triggers in such a way that they map the turn of the year and thus roughly indicate to the user via the version which adjustments have been released: Version Reasons MAJOR planning year gets increased, controlling year gets increased MINOR new features get integrated PATCH bugfixing","title":"Step by step"},{"location":"concepts/versioning_with_semver/#extensions","text":"Semantic versioning is not the only way to design software versions. It remains important that version numbers remain predictable and that references to features and fixes can be made.","title":"Extensions"},{"location":"concepts/write_concepts/","text":"Motivation Concepts are an important step in distributing work flows in our LOB and in simplifying documentation of cross-cutting concerns. This concept describes how concepts should be structured in order to homogenise the work with them and to meet the expectations of the readers. This increases the degree of reusability and allows you to use concepts from this repository in your project as guidance and documentation. Furthermore, concepts from projects can be distributed in this repository. Context For the structural design of concepts, we use the four quadrant model . As the name suggests, the model divides content into four questions to be answered: Why? What? How to? What else? The following table shows possible topics or possibilities to answer the questions. It is important that the order of the questions should not be changed in your writing. Quadrant Topics\\possibilities Why purposes, motivate, raise awareness What explaining terms, creating a background, go into detail How to examples, give concrete instructions, tutorials What else alternative solutions, looking ahead, related topics To meet expectations, first-level headings should be similar in all concepts. Of course, deeper level headings should have a technical reference. The following are possible headings for the individual quadrants. Quadrant Headings Why motivation, task What context, influencing factors, alternatives, solution, background How to example, application, step by step What else next steps, extensions, feedback, outlook At the start of the document, a section for meta-information is defined, in which the author, title, and a short summary are specified. --- title : My Document summary : A brief description of my document. authors : - Alexander Serowy - Tom Tucker tags : - concept --- Step by step In our example we write a concept to document a microservices architecture. In the first step we write the motivation why the concept was created in the first place. \"Documentation is often an uninvited guest to developers. In order to increase the acceptance of up-to-date documentation, we provide a slightly weighty way to capture these systems...\" After the motivation, we present in our concept how exactly the documentation is structured and which content elements we should describe. In addition, background and terminology are described and specified here as well. Next comes the \"How to\". In this section we will then build up a documentation about an exemplary microservices architecture. We describe how the architecture that is to be described looks like and then go step by step through the individual structures that were defined and explained in the previous step. In the last step, we then write about related topics and alternative solutions, such as how decisions can be written and filed differently like ADR (Architecture Decision Record). Next steps Using this template we can achieve a high degree of reusability for concepts. If you use concepts as documentation in projects, you are welcome to make them available to everyone in this repository!","title":"How to write concepts"},{"location":"concepts/write_concepts/#motivation","text":"Concepts are an important step in distributing work flows in our LOB and in simplifying documentation of cross-cutting concerns. This concept describes how concepts should be structured in order to homogenise the work with them and to meet the expectations of the readers. This increases the degree of reusability and allows you to use concepts from this repository in your project as guidance and documentation. Furthermore, concepts from projects can be distributed in this repository.","title":"Motivation"},{"location":"concepts/write_concepts/#context","text":"For the structural design of concepts, we use the four quadrant model . As the name suggests, the model divides content into four questions to be answered: Why? What? How to? What else? The following table shows possible topics or possibilities to answer the questions. It is important that the order of the questions should not be changed in your writing. Quadrant Topics\\possibilities Why purposes, motivate, raise awareness What explaining terms, creating a background, go into detail How to examples, give concrete instructions, tutorials What else alternative solutions, looking ahead, related topics To meet expectations, first-level headings should be similar in all concepts. Of course, deeper level headings should have a technical reference. The following are possible headings for the individual quadrants. Quadrant Headings Why motivation, task What context, influencing factors, alternatives, solution, background How to example, application, step by step What else next steps, extensions, feedback, outlook At the start of the document, a section for meta-information is defined, in which the author, title, and a short summary are specified. --- title : My Document summary : A brief description of my document. authors : - Alexander Serowy - Tom Tucker tags : - concept ---","title":"Context"},{"location":"concepts/write_concepts/#step-by-step","text":"In our example we write a concept to document a microservices architecture. In the first step we write the motivation why the concept was created in the first place. \"Documentation is often an uninvited guest to developers. In order to increase the acceptance of up-to-date documentation, we provide a slightly weighty way to capture these systems...\" After the motivation, we present in our concept how exactly the documentation is structured and which content elements we should describe. In addition, background and terminology are described and specified here as well. Next comes the \"How to\". In this section we will then build up a documentation about an exemplary microservices architecture. We describe how the architecture that is to be described looks like and then go step by step through the individual structures that were defined and explained in the previous step. In the last step, we then write about related topics and alternative solutions, such as how decisions can be written and filed differently like ADR (Architecture Decision Record).","title":"Step by step"},{"location":"concepts/write_concepts/#next-steps","text":"Using this template we can achieve a high degree of reusability for concepts. If you use concepts as documentation in projects, you are welcome to make them available to everyone in this repository!","title":"Next steps"},{"location":"concepts/templates/","text":"","title":"Templates"},{"location":"concepts/templates/template_architectural_decisions/","text":"--- title : Decision A summary : A brief description of question and decision. authors : - Alexander Serowy - Tom Tucker tags : - decision --- ## Question > What exactly is the problem? > Why is it relevant to architecture? > What are the effects of the decision? ## Influencing factors > Which fixed framework conditions do we have to comply with? > Which architectural goals are to be considered? > Which risks are affected? ## Assumptions > What assumptions have we made? > Which assumptions can be tested in advance and how? > What new risks do we have to expect? ## Alternatives > Which solution options do we shortlist? > How do we evaluate each one? > Which options do we exclude? ## Decision > Who made the decision or was involved in it? > What is the decision? > What is the reasoning behind it? > When was the decision made?","title":"Architectural decision"},{"location":"design_pattern/","text":"","title":"Design pattern"},{"location":"design_pattern/behavioral/","text":"","title":"Behavioral"},{"location":"design_pattern/behavioral/chain_of_responsibility/","text":"Sample implementations Registration with ms dependency injection container ( source ) public void ConfigureServices ( IServiceCollection services ) { services . Chain < IChainOfResponsibility >() . Add < HandlerOne >() . Add < HandlerTwo >() . Configure (); } public static class ChainConfigurator { public static IChainConfigurator < T > Chain < T >( this IServiceCollection services ) where T : class { return new ChainConfiguratorImpl < T >( services ); } public interface IChainConfigurator < T > { IChainConfigurator < T > Add < TImplementation >() where TImplementation : T ; void Configure (); } private class ChainConfiguratorImpl < T > : IChainConfigurator < T > where T : class { private readonly IServiceCollection _services ; private List < Type > _types ; private Type _interfaceType ; public ChainConfiguratorImpl ( IServiceCollection services ) { _services = services ; _types = new List < Type >(); _interfaceType = typeof ( T ); } public IChainConfigurator < T > Add < TImplementation >() where TImplementation : T { var type = typeof ( TImplementation ); _types . Add ( type ); return this ; } public void Configure () { if ( _types . Count == 0 ) throw new InvalidOperationException ( $ \"No implementation defined for {_interfaceType.Name}\" ); foreach ( var type in _types ) { ConfigureType ( type ); } } private void ConfigureType ( Type currentType ) { // gets the next type, as that will be injected in the current type var nextType = _types . SkipWhile ( x => x != currentType ) . SkipWhile ( x => x == currentType ) . FirstOrDefault (); // Makes a parameter expression, that is the IServiceProvider x var parameter = Expression . Parameter ( typeof ( IServiceProvider ), \"x\" ); // get constructor with highest number of parameters. Ideally, there should be only 1 // constructor, but better be safe. var ctor = currentType . GetConstructors () . OrderByDescending ( x => x . GetParameters (). Count ()) . First (); // for each parameter in the constructor var ctorParameters = ctor . GetParameters (). Select ( p => { // check if it implements the interface. That's how we find which parameter to inject // the next handler. if ( _interfaceType . IsAssignableFrom ( p . ParameterType )) { if ( nextType is null ) { // if there's no next type, current type is the last in the chain, so it just // receives null return Expression . Constant ( null , _interfaceType ); } else { // if there is, then we call IServiceProvider.GetRequiredService to resolve next // type for us return Expression . Call ( typeof ( ServiceProviderServiceExtensions ), \"GetRequiredService\" , new Type [] { nextType }, parameter ); } } // this is a parameter we don't care about, so we just ask GetRequiredService to // resolve it for us return ( Expression ) Expression . Call ( typeof ( ServiceProviderServiceExtensions ), \"GetRequiredService\" , new Type [] { p . ParameterType }, parameter ); }); // cool, we have all of our constructors parameters set, so we build a \"new\" // expression to invoke it. var body = Expression . New ( ctor , ctorParameters . ToArray ()); // if current type is the first in our list, then we register it by the interface, otherwise // by the concrete type var first = _types [ 0 ] == currentType ; var resolveType = first ? _interfaceType : currentType ; var expressionType = Expression . GetFuncType ( typeof ( IServiceProvider ), resolveType ); // finally, we can build our expression var expression = Expression . Lambda ( expressionType , body , parameter ); // compile it var compiledExpression = ( Func < IServiceProvider , object >) expression . Compile (); // and register it in the services collection as transient _services . AddTransient ( resolveType , compiledExpression ); } } }","title":"Chain of responsibility pattern"},{"location":"design_pattern/behavioral/chain_of_responsibility/#sample-implementations","text":"","title":"Sample implementations"},{"location":"design_pattern/behavioral/chain_of_responsibility/#registration-with-ms-dependency-injection-container-source","text":"public void ConfigureServices ( IServiceCollection services ) { services . Chain < IChainOfResponsibility >() . Add < HandlerOne >() . Add < HandlerTwo >() . Configure (); } public static class ChainConfigurator { public static IChainConfigurator < T > Chain < T >( this IServiceCollection services ) where T : class { return new ChainConfiguratorImpl < T >( services ); } public interface IChainConfigurator < T > { IChainConfigurator < T > Add < TImplementation >() where TImplementation : T ; void Configure (); } private class ChainConfiguratorImpl < T > : IChainConfigurator < T > where T : class { private readonly IServiceCollection _services ; private List < Type > _types ; private Type _interfaceType ; public ChainConfiguratorImpl ( IServiceCollection services ) { _services = services ; _types = new List < Type >(); _interfaceType = typeof ( T ); } public IChainConfigurator < T > Add < TImplementation >() where TImplementation : T { var type = typeof ( TImplementation ); _types . Add ( type ); return this ; } public void Configure () { if ( _types . Count == 0 ) throw new InvalidOperationException ( $ \"No implementation defined for {_interfaceType.Name}\" ); foreach ( var type in _types ) { ConfigureType ( type ); } } private void ConfigureType ( Type currentType ) { // gets the next type, as that will be injected in the current type var nextType = _types . SkipWhile ( x => x != currentType ) . SkipWhile ( x => x == currentType ) . FirstOrDefault (); // Makes a parameter expression, that is the IServiceProvider x var parameter = Expression . Parameter ( typeof ( IServiceProvider ), \"x\" ); // get constructor with highest number of parameters. Ideally, there should be only 1 // constructor, but better be safe. var ctor = currentType . GetConstructors () . OrderByDescending ( x => x . GetParameters (). Count ()) . First (); // for each parameter in the constructor var ctorParameters = ctor . GetParameters (). Select ( p => { // check if it implements the interface. That's how we find which parameter to inject // the next handler. if ( _interfaceType . IsAssignableFrom ( p . ParameterType )) { if ( nextType is null ) { // if there's no next type, current type is the last in the chain, so it just // receives null return Expression . Constant ( null , _interfaceType ); } else { // if there is, then we call IServiceProvider.GetRequiredService to resolve next // type for us return Expression . Call ( typeof ( ServiceProviderServiceExtensions ), \"GetRequiredService\" , new Type [] { nextType }, parameter ); } } // this is a parameter we don't care about, so we just ask GetRequiredService to // resolve it for us return ( Expression ) Expression . Call ( typeof ( ServiceProviderServiceExtensions ), \"GetRequiredService\" , new Type [] { p . ParameterType }, parameter ); }); // cool, we have all of our constructors parameters set, so we build a \"new\" // expression to invoke it. var body = Expression . New ( ctor , ctorParameters . ToArray ()); // if current type is the first in our list, then we register it by the interface, otherwise // by the concrete type var first = _types [ 0 ] == currentType ; var resolveType = first ? _interfaceType : currentType ; var expressionType = Expression . GetFuncType ( typeof ( IServiceProvider ), resolveType ); // finally, we can build our expression var expression = Expression . Lambda ( expressionType , body , parameter ); // compile it var compiledExpression = ( Func < IServiceProvider , object >) expression . Compile (); // and register it in the services collection as transient _services . AddTransient ( resolveType , compiledExpression ); } } }","title":"Registration with ms dependency injection container (source)"},{"location":"design_pattern/behavioral/strategy_pattern/","text":"Sample implementations Coupling with factory and di The given strategy interface contains two methods. ValidFor describes - in this example with an enum - if the strategy should be used. This validation could be a predicate for example as well. GetDistanceInSecondsAsync describes the encapsulated business logic for this concrete strategy. internal interface IDistanceQueryStrategy { FirstResponderWayResolverStrategy ValidFor { get ; } Task < DistanceResult > GetDistanceInSecondsAsync ( CoordinateDto origin , params CoordinateDto [] waypoints ); } With dependency injection, we bind multiple concrete strategies to the same interface. Thus, we can get a complete collection of all concrete implementations with one call in our service. services . AddTransient < IDistanceQuery , DistanceQuery >(); services . AddTransient < IDistanceQueryStrategy , GoogleDistanceQueryStrategy >(); services . AddTransient < IDistanceQueryStrategy , LinearDistanceQueryStrategy >(); In the given example a default strategy is configured to enable a robust behavior. In addition the concrete enum value is method injected. This could also be addressed within the query, to reduce the method signature (as long as we have additional information regarding the resolvement). internal class DistanceQuery : IDistanceQuery { private readonly IDictionary < FirstResponderWayResolverStrategy , IDistanceQueryStrategy > _strategies ; public DistanceQuery ( IEnumerable < IDistanceQueryStrategy > strategies ) { _strategies = strategies . ToDictionary ( strategy => strategy . ValidFor , strategy => strategy ); } public Task < DistanceResult > GetDistanceInSecondsAsync ( FirstResponderWayResolverStrategy strategy , CoordinateDto origin , params CoordinateDto [] waypoints ) { var strategyToUse = FirstResponderWayResolverStrategy . GoogleDistanceResolver if ( _strategies . ContainsKey ( strategy )) { strategyToUse = strategy ; } return _strategies [ strategyToUse ]. GetDistanceInSecondsAsync ( origin , waypoints ); } } Method injection The interface is used to abstract concrete implementations. public interface IStepStrategy { IVersion Version { get ; } int Index { get ; } public Task < IEnumerable < ICommand >> RunAsync ( IEvent event ); } With this abstraction a concrete implementation for this logic can be injected at runtime. public async Task < IEnumerable < ICommand >> RunAsync ( IEvent event , IStepStrategy step ) { var commands = await step . RunAsync ( event ) . ConfigureAwait ( false ); if ( commands is null ) { return Enumerable . Empty < ICommand >(); } await _store . AddReferencesAsync ( commands ) . ConfigureAwait ( false ); return commands ; } Further reads https://www.oodesign.com/strategy-pattern.html","title":"Strategy pattern"},{"location":"design_pattern/behavioral/strategy_pattern/#sample-implementations","text":"","title":"Sample implementations"},{"location":"design_pattern/behavioral/strategy_pattern/#coupling-with-factory-and-di","text":"The given strategy interface contains two methods. ValidFor describes - in this example with an enum - if the strategy should be used. This validation could be a predicate for example as well. GetDistanceInSecondsAsync describes the encapsulated business logic for this concrete strategy. internal interface IDistanceQueryStrategy { FirstResponderWayResolverStrategy ValidFor { get ; } Task < DistanceResult > GetDistanceInSecondsAsync ( CoordinateDto origin , params CoordinateDto [] waypoints ); } With dependency injection, we bind multiple concrete strategies to the same interface. Thus, we can get a complete collection of all concrete implementations with one call in our service. services . AddTransient < IDistanceQuery , DistanceQuery >(); services . AddTransient < IDistanceQueryStrategy , GoogleDistanceQueryStrategy >(); services . AddTransient < IDistanceQueryStrategy , LinearDistanceQueryStrategy >(); In the given example a default strategy is configured to enable a robust behavior. In addition the concrete enum value is method injected. This could also be addressed within the query, to reduce the method signature (as long as we have additional information regarding the resolvement). internal class DistanceQuery : IDistanceQuery { private readonly IDictionary < FirstResponderWayResolverStrategy , IDistanceQueryStrategy > _strategies ; public DistanceQuery ( IEnumerable < IDistanceQueryStrategy > strategies ) { _strategies = strategies . ToDictionary ( strategy => strategy . ValidFor , strategy => strategy ); } public Task < DistanceResult > GetDistanceInSecondsAsync ( FirstResponderWayResolverStrategy strategy , CoordinateDto origin , params CoordinateDto [] waypoints ) { var strategyToUse = FirstResponderWayResolverStrategy . GoogleDistanceResolver if ( _strategies . ContainsKey ( strategy )) { strategyToUse = strategy ; } return _strategies [ strategyToUse ]. GetDistanceInSecondsAsync ( origin , waypoints ); } }","title":"Coupling with factory and di"},{"location":"design_pattern/behavioral/strategy_pattern/#method-injection","text":"The interface is used to abstract concrete implementations. public interface IStepStrategy { IVersion Version { get ; } int Index { get ; } public Task < IEnumerable < ICommand >> RunAsync ( IEvent event ); } With this abstraction a concrete implementation for this logic can be injected at runtime. public async Task < IEnumerable < ICommand >> RunAsync ( IEvent event , IStepStrategy step ) { var commands = await step . RunAsync ( event ) . ConfigureAwait ( false ); if ( commands is null ) { return Enumerable . Empty < ICommand >(); } await _store . AddReferencesAsync ( commands ) . ConfigureAwait ( false ); return commands ; }","title":"Method injection"},{"location":"design_pattern/behavioral/strategy_pattern/#further-reads","text":"https://www.oodesign.com/strategy-pattern.html","title":"Further reads"},{"location":"design_pattern/creational/","text":"","title":"Creational"},{"location":"design_pattern/creational/builder_pattern/","text":"Sample implementation The given implementation abuses the builder pattern for validation. Thus, you can easily chain validation steps and reuse the configured validator object. public class Validator < TClass > { private IList < Predicate < TClass >> _rules = new List < Predicate < TClass >>(); public Validator < TClass > AddRule ( Predicate < TClass > rule ) { _rules . Add ( rule ); return this ; } public bool Validate ( TClass obj ) { foreach ( var func in _rules ) { if (! func ( obj )) { return false ; } } return true ; } } Further reads https://www.pmichaels.net/2018/01/27/using-builder-pattern-validation/","title":"Builder pattern"},{"location":"design_pattern/creational/builder_pattern/#sample-implementation","text":"The given implementation abuses the builder pattern for validation. Thus, you can easily chain validation steps and reuse the configured validator object. public class Validator < TClass > { private IList < Predicate < TClass >> _rules = new List < Predicate < TClass >>(); public Validator < TClass > AddRule ( Predicate < TClass > rule ) { _rules . Add ( rule ); return this ; } public bool Validate ( TClass obj ) { foreach ( var func in _rules ) { if (! func ( obj )) { return false ; } } return true ; } }","title":"Sample implementation"},{"location":"design_pattern/creational/builder_pattern/#further-reads","text":"https://www.pmichaels.net/2018/01/27/using-builder-pattern-validation/","title":"Further reads"},{"location":"design_pattern/functional/","text":"","title":"Functional"},{"location":"design_pattern/functional/composition_pattern/","text":"Reference implementation Because of a narrow support for fp in c#, we need a helper to build a proper composition of functions. public static Func < T , TNextOut > Compose < T , TInitOut , TNextOut >( this Func < T , TInitOut > initial , Func < TInitOut , TNextOut > next ) { return x => next ( initial ( x )); } With the given extension you can compose functions with output equals input. public async Task < IStepConfiguration > ResolveAsync ( IDictionary < IVersion , IList < IStepConfiguration >> versions ) { var resolverFunc = GetVersionFunc (). Compose ( GetStepFunc ()); return resolverFunc ( versions ); } private static readonly Func < IDictionary < IVersion , IList < IStepConfiguration >>, IList < IStepConfiguration >> GetVersionFunc = versions => versions . FirstOrDefault (). Value ; private static readonly Func < IList < IStepConfiguration >, IStepConfiguration > GetStepFunc = steps => steps ?. FirstOrDefault (); For functions with multiple inputs you can use Currying to reduce the count of inputs to one. Further reads http://hamidmosalla.com/2019/04/25/functional-programming-in-c-sharp-a-brief-guide/","title":"Composition pattern"},{"location":"design_pattern/functional/composition_pattern/#reference-implementation","text":"Because of a narrow support for fp in c#, we need a helper to build a proper composition of functions. public static Func < T , TNextOut > Compose < T , TInitOut , TNextOut >( this Func < T , TInitOut > initial , Func < TInitOut , TNextOut > next ) { return x => next ( initial ( x )); } With the given extension you can compose functions with output equals input. public async Task < IStepConfiguration > ResolveAsync ( IDictionary < IVersion , IList < IStepConfiguration >> versions ) { var resolverFunc = GetVersionFunc (). Compose ( GetStepFunc ()); return resolverFunc ( versions ); } private static readonly Func < IDictionary < IVersion , IList < IStepConfiguration >>, IList < IStepConfiguration >> GetVersionFunc = versions => versions . FirstOrDefault (). Value ; private static readonly Func < IList < IStepConfiguration >, IStepConfiguration > GetStepFunc = steps => steps ?. FirstOrDefault (); For functions with multiple inputs you can use Currying to reduce the count of inputs to one.","title":"Reference implementation"},{"location":"design_pattern/functional/composition_pattern/#further-reads","text":"http://hamidmosalla.com/2019/04/25/functional-programming-in-c-sharp-a-brief-guide/","title":"Further reads"},{"location":"design_pattern/functional/currying_pattern/","text":"With Currying you can reduce the number of inputs of a function. This can get handy to e.g. reduce inputs to one and be able to compose different functions. private static readonly Func < IVersionResolver , Func < IDictionary < IVersion , IList < IStepConfiguration >>, IList < IStepConfiguration >>> GetVersionFunc = versionResolver => ( versions ) => versionResolver . GetVersion ( versions );","title":"Currying pattern"},{"location":"design_pattern/resource_oriented/","text":"","title":"Resource oriented"},{"location":"design_pattern/resource_oriented/dispose_pattern/","text":"How to dispose 'unmanaged' in C# Handling dispose of handles and managed objects. The pattern uses IDisposable and ensures a clean way to handle dispositions and deconstructions. using System ; using System.ComponentModel ; // The following example demonstrates how to create // a resource class that implements the IDisposable interface // and the IDisposable.Dispose method. public class DisposeExample { // A base class that implements IDisposable. // By implementing IDisposable, you are announcing that // instances of this type allocate scarce resources. public class MyResource : IDisposable { // Pointer to an external unmanaged resource. private IntPtr _handle ; // Other managed resource this class uses. private Component _component = new Component (); // Track whether Dispose has been called. private bool _disposed = false ; // The class constructor. public MyResource ( IntPtr handle ) { _handle = handle ; } // Implement IDisposable. // Do not make this method virtual. // A derived class should not be able to override this method. public void Dispose () { Dispose ( true ); // This object will be cleaned up by the Dispose method. // Therefore, you should call GC.SupressFinalize to // take this object off the finalization queue // and prevent finalization code for this object // from executing a second time. GC . SuppressFinalize ( this ); } // Dispose(bool disposing) executes in two distinct scenarios. // If disposing equals true, the method has been called directly // or indirectly by a user's code. Managed and unmanaged resources // can be disposed. // If disposing equals false, the method has been called by the // runtime from inside the finalizer and you should not reference // other objects. Only unmanaged resources can be disposed. protected virtual void Dispose ( bool disposing ) { // Check to see if Dispose has already been called. if (! _disposed ) { // If disposing equals true, dispose all managed // and unmanaged resources. if ( disposing ) { // Dispose managed resources. _component . Dispose (); } // Call the appropriate methods to clean up // unmanaged resources here. // If disposing is false, // only the following code is executed. CloseHandle ( _handle ); _handle = IntPtr . Zero ; // Note disposing has been done. _disposed = true ; } } // Use interop to call the method necessary // to clean up the unmanaged resource. [System.Runtime.InteropServices.DllImport(\"Kernel32\")] private extern static Boolean CloseHandle ( IntPtr handle ); // Use C# destructor syntax for finalization code. // This destructor will run only if the Dispose method // does not get called. // It gives your base class the opportunity to finalize. // Do not provide destructors in types derived from this class. ~ MyResource () { // Do not re-create Dispose clean-up code here. // Calling Dispose(false) is optimal in terms of // readability and maintainability. Dispose ( false ); } } }","title":"Dispose pattern"},{"location":"design_pattern/resource_oriented/dispose_pattern/#how-to-dispose-unmanaged-in-c","text":"Handling dispose of handles and managed objects. The pattern uses IDisposable and ensures a clean way to handle dispositions and deconstructions. using System ; using System.ComponentModel ; // The following example demonstrates how to create // a resource class that implements the IDisposable interface // and the IDisposable.Dispose method. public class DisposeExample { // A base class that implements IDisposable. // By implementing IDisposable, you are announcing that // instances of this type allocate scarce resources. public class MyResource : IDisposable { // Pointer to an external unmanaged resource. private IntPtr _handle ; // Other managed resource this class uses. private Component _component = new Component (); // Track whether Dispose has been called. private bool _disposed = false ; // The class constructor. public MyResource ( IntPtr handle ) { _handle = handle ; } // Implement IDisposable. // Do not make this method virtual. // A derived class should not be able to override this method. public void Dispose () { Dispose ( true ); // This object will be cleaned up by the Dispose method. // Therefore, you should call GC.SupressFinalize to // take this object off the finalization queue // and prevent finalization code for this object // from executing a second time. GC . SuppressFinalize ( this ); } // Dispose(bool disposing) executes in two distinct scenarios. // If disposing equals true, the method has been called directly // or indirectly by a user's code. Managed and unmanaged resources // can be disposed. // If disposing equals false, the method has been called by the // runtime from inside the finalizer and you should not reference // other objects. Only unmanaged resources can be disposed. protected virtual void Dispose ( bool disposing ) { // Check to see if Dispose has already been called. if (! _disposed ) { // If disposing equals true, dispose all managed // and unmanaged resources. if ( disposing ) { // Dispose managed resources. _component . Dispose (); } // Call the appropriate methods to clean up // unmanaged resources here. // If disposing is false, // only the following code is executed. CloseHandle ( _handle ); _handle = IntPtr . Zero ; // Note disposing has been done. _disposed = true ; } } // Use interop to call the method necessary // to clean up the unmanaged resource. [System.Runtime.InteropServices.DllImport(\"Kernel32\")] private extern static Boolean CloseHandle ( IntPtr handle ); // Use C# destructor syntax for finalization code. // This destructor will run only if the Dispose method // does not get called. // It gives your base class the opportunity to finalize. // Do not provide destructors in types derived from this class. ~ MyResource () { // Do not re-create Dispose clean-up code here. // Calling Dispose(false) is optimal in terms of // readability and maintainability. Dispose ( false ); } } }","title":"How to dispose 'unmanaged' in C#"},{"location":"design_pattern/structural/","text":"","title":"Structural"},{"location":"design_pattern/structural/type_safe_enum_pattern/","text":"Reference implementation [System.Diagnostics.CodeAnalysis.SuppressMessage( \"Design\", \"CA1036:Override methods on comparable types\", Justification = \"Since Enumeration is abstract, different concrete implementations must not be compareable.\")] public abstract class Enumeration : IComparable { public string Name { get ; private set ; } public int Id { get ; private set ; } protected Enumeration ( int id , string name ) { Id = id ; Name = name ; } public override string ToString () { return Name ; } public static IEnumerable < T > GetAll < T >() where T : Enumeration { var fields = typeof ( T ). GetFields ( BindingFlags . Public | BindingFlags . Static | BindingFlags . DeclaredOnly ); return fields . Select ( fld => fld . GetValue ( null )). Cast < T >(); } public static T FromString < T >( string name ) where T : Enumeration { return GetAll < T >(). Single ( enmrtn => string . Equals ( enmrtn . Name , name , StringComparison . OrdinalIgnoreCase )); } public static T FromValue < T >( int value ) where T : Enumeration { return GetAll < T >(). Single ( enmrtn => enmrtn . Name . Equals ( value )); } public int CompareTo ( object other ) { if (!( other is Enumeration otherValue )) { throw new InvalidCastException ( $ \"{nameof(other)} is not of type {nameof(Enumeration)}.\" ); } if ( GetType (). Equals ( otherValue . GetType ())) { throw new InvalidCastException ( $ \"{nameof(otherValue)} is not of type {GetType().Name}.\" ); } return Id . CompareTo ( otherValue . Id ); } public override bool Equals ( object obj ) { if (!( obj is Enumeration otherValue )) { return false ; } var typeMatches = GetType (). Equals ( otherValue . GetType ()); var valueMatches = Id . Equals ( otherValue . Id ); return typeMatches && valueMatches ; } public override int GetHashCode () { return Id ; } } Usage Further reads https://docs.microsoft.com/en-us/dotnet/architecture/microservices/microservice-ddd-cqrs-patterns/enumeration-classes-over-enum-types https://lostechies.com/jimmybogard/2008/08/12/enumeration-classes/ https://ardalis.com/enum-alternatives-in-c","title":"Type safe enum pattern"},{"location":"design_pattern/structural/type_safe_enum_pattern/#reference-implementation","text":"[System.Diagnostics.CodeAnalysis.SuppressMessage( \"Design\", \"CA1036:Override methods on comparable types\", Justification = \"Since Enumeration is abstract, different concrete implementations must not be compareable.\")] public abstract class Enumeration : IComparable { public string Name { get ; private set ; } public int Id { get ; private set ; } protected Enumeration ( int id , string name ) { Id = id ; Name = name ; } public override string ToString () { return Name ; } public static IEnumerable < T > GetAll < T >() where T : Enumeration { var fields = typeof ( T ). GetFields ( BindingFlags . Public | BindingFlags . Static | BindingFlags . DeclaredOnly ); return fields . Select ( fld => fld . GetValue ( null )). Cast < T >(); } public static T FromString < T >( string name ) where T : Enumeration { return GetAll < T >(). Single ( enmrtn => string . Equals ( enmrtn . Name , name , StringComparison . OrdinalIgnoreCase )); } public static T FromValue < T >( int value ) where T : Enumeration { return GetAll < T >(). Single ( enmrtn => enmrtn . Name . Equals ( value )); } public int CompareTo ( object other ) { if (!( other is Enumeration otherValue )) { throw new InvalidCastException ( $ \"{nameof(other)} is not of type {nameof(Enumeration)}.\" ); } if ( GetType (). Equals ( otherValue . GetType ())) { throw new InvalidCastException ( $ \"{nameof(otherValue)} is not of type {GetType().Name}.\" ); } return Id . CompareTo ( otherValue . Id ); } public override bool Equals ( object obj ) { if (!( obj is Enumeration otherValue )) { return false ; } var typeMatches = GetType (). Equals ( otherValue . GetType ()); var valueMatches = Id . Equals ( otherValue . Id ); return typeMatches && valueMatches ; } public override int GetHashCode () { return Id ; } }","title":"Reference implementation"},{"location":"design_pattern/structural/type_safe_enum_pattern/#usage","text":"","title":"Usage"},{"location":"design_pattern/structural/type_safe_enum_pattern/#further-reads","text":"https://docs.microsoft.com/en-us/dotnet/architecture/microservices/microservice-ddd-cqrs-patterns/enumeration-classes-over-enum-types https://lostechies.com/jimmybogard/2008/08/12/enumeration-classes/ https://ardalis.com/enum-alternatives-in-c","title":"Further reads"},{"location":"devops/","text":"","title":"Devops"},{"location":"devops/azure/","text":"Common mistakes Predefined variables Using predefined values in variables will not get resolved for name. The following snipped will resolve in \"$(Build.SourceBranchName).11\" in dev ops. name : $(majorMinorVersion).$(patchVersion) variables : majorMinorVersion : '$(Build.SourceBranchName)' patchVersion : $[counter(variables['majorMinorVersion'], 10)] Because variables will get determined at runtime (except expressions) you should use the predefined variable directly. Thus, the resolved name on a branch named \"1.0\" would be \"1.0.11\". name : $(Build.SourceBranchName).$(patchVersion) variables : majorMinorVersion : '$(Build.SourceBranchName)' patchVersion : $[counter(variables['majorMinorVersion'], 10)] Links Nuget https://docs.microsoft.com/en-us/azure/devops/pipelines/artifacts/nuget?view=azure-devops&tabs=yaml https://medium.com/@dan.cokely/creating-nuget-packages-in-azure-devops-with-azure-pipelines-and-yaml-d6fa30f0f15e Tags How to use tags as variable in yaml. Testing Testing azure functions in CI/CD Versioning Why you should not use r:rev as patch.","title":"Azure DevOps"},{"location":"devops/azure/#common-mistakes","text":"","title":"Common mistakes"},{"location":"devops/azure/#predefined-variables","text":"Using predefined values in variables will not get resolved for name. The following snipped will resolve in \"$(Build.SourceBranchName).11\" in dev ops. name : $(majorMinorVersion).$(patchVersion) variables : majorMinorVersion : '$(Build.SourceBranchName)' patchVersion : $[counter(variables['majorMinorVersion'], 10)] Because variables will get determined at runtime (except expressions) you should use the predefined variable directly. Thus, the resolved name on a branch named \"1.0\" would be \"1.0.11\". name : $(Build.SourceBranchName).$(patchVersion) variables : majorMinorVersion : '$(Build.SourceBranchName)' patchVersion : $[counter(variables['majorMinorVersion'], 10)]","title":"Predefined variables"},{"location":"devops/azure/#links","text":"Nuget https://docs.microsoft.com/en-us/azure/devops/pipelines/artifacts/nuget?view=azure-devops&tabs=yaml https://medium.com/@dan.cokely/creating-nuget-packages-in-azure-devops-with-azure-pipelines-and-yaml-d6fa30f0f15e Tags How to use tags as variable in yaml. Testing Testing azure functions in CI/CD Versioning Why you should not use r:rev as patch.","title":"Links"},{"location":"principles/","text":"What is solid SOLID is an acronym published by Robert C. Martin and consists of five principles tackling different problems in software development. All five principles can be unstood as subjective guidelines for coding in object oriented and functional programming. Some programmers like me are riding the cult train because of many upsides. But even if it is cult, it is damn simple to programm SOLID. Why should we use it SOLID leads to smaller pieces of code and enhances expectations regarding inheritance and api design. These enhancements lead to the following advantages: Better readability Enhanced fulfilment of expectations Less merge conflicts in bigger teams Increased ability to reuse code More specific designations Increased maintainability Enhanced flexibility Unit-testability Principles Single responsibility principle Open closed principle Liskov substitution principle Interface segregation principle Dependency inversion principle","title":"Principles"},{"location":"principles/#what-is-solid","text":"SOLID is an acronym published by Robert C. Martin and consists of five principles tackling different problems in software development. All five principles can be unstood as subjective guidelines for coding in object oriented and functional programming. Some programmers like me are riding the cult train because of many upsides. But even if it is cult, it is damn simple to programm SOLID.","title":"What is solid"},{"location":"principles/#why-should-we-use-it","text":"SOLID leads to smaller pieces of code and enhances expectations regarding inheritance and api design. These enhancements lead to the following advantages: Better readability Enhanced fulfilment of expectations Less merge conflicts in bigger teams Increased ability to reuse code More specific designations Increased maintainability Enhanced flexibility Unit-testability","title":"Why should we use it"},{"location":"principles/#principles","text":"Single responsibility principle Open closed principle Liskov substitution principle Interface segregation principle Dependency inversion principle","title":"Principles"},{"location":"principles/dependency_inversion_principle/","text":"Definition Dependency Inversion is the strategy of depending upon interfaces or abstract functions and classes, rather than upon concrete functions and classes. This principle is the enabling force behind component design. No dependency should target a concrete class. - Robert C. Martin Guidelines Use dependency injection (IoC) with interfaces Use only construcor injection (where applicable) Only use \"new\" in your methods for logic containing classes if control over the object life cycle is needed Avoid the service locator anti pattern","title":"Dependency inversion principle"},{"location":"principles/dependency_inversion_principle/#definition","text":"Dependency Inversion is the strategy of depending upon interfaces or abstract functions and classes, rather than upon concrete functions and classes. This principle is the enabling force behind component design. No dependency should target a concrete class. - Robert C. Martin","title":"Definition"},{"location":"principles/dependency_inversion_principle/#guidelines","text":"Use dependency injection (IoC) with interfaces Use only construcor injection (where applicable) Only use \"new\" in your methods for logic containing classes if control over the object life cycle is needed Avoid the service locator anti pattern","title":"Guidelines"},{"location":"principles/interface_segregation_principle/","text":"Definition ISP splits interfaces that are very large into smaller and more specific ones so that clients will only have to know about the methods that are of interest to them. Such shrunken interfaces are also called role interfaces. ISP is intended to keep a system decoupled and thus easier to refactor, change, and redeploy. [...] no client should be forced to depend on methods it does not use. - Robert C. Martin Guidelines Avoid big interfaces like eight methods and more (Every now and then there are useful exceptions) Try to implement interfaces with LSP and SRP in mind","title":"Interface segregation principle"},{"location":"principles/interface_segregation_principle/#definition","text":"ISP splits interfaces that are very large into smaller and more specific ones so that clients will only have to know about the methods that are of interest to them. Such shrunken interfaces are also called role interfaces. ISP is intended to keep a system decoupled and thus easier to refactor, change, and redeploy. [...] no client should be forced to depend on methods it does not use. - Robert C. Martin","title":"Definition"},{"location":"principles/interface_segregation_principle/#guidelines","text":"Avoid big interfaces like eight methods and more (Every now and then there are useful exceptions) Try to implement interfaces with LSP and SRP in mind","title":"Guidelines"},{"location":"principles/liskov_substitution_principle/","text":"Definition If S is a subtype of T, then objects of type T may be replaced with objects of type S (i.e. an object of type T may be substituted with any object of a subtype S) without altering any of the desirable properties of the program (correctness, task performed, etc.). It is a semantic rather than merely syntactic relation, because it intends to guarantee semantic interoperability of types in a hierarchy, object types in particular. Prominent violation System.Array implements the ICollection\\<T> interface. Calling Add() throws a NotSupportedException at runtime. Guidelines Ask yourself the following questions when implementing interfaces/base classes:\u200b Is every member/method suitable for each concrete implementation?\u200b Is the naming of the interface/base class open enough for later implementations?","title":"Liskov substitution principle"},{"location":"principles/liskov_substitution_principle/#definition","text":"If S is a subtype of T, then objects of type T may be replaced with objects of type S (i.e. an object of type T may be substituted with any object of a subtype S) without altering any of the desirable properties of the program (correctness, task performed, etc.). It is a semantic rather than merely syntactic relation, because it intends to guarantee semantic interoperability of types in a hierarchy, object types in particular.","title":"Definition"},{"location":"principles/liskov_substitution_principle/#prominent-violation","text":"System.Array implements the ICollection\\<T> interface. Calling Add() throws a NotSupportedException at runtime.","title":"Prominent violation"},{"location":"principles/liskov_substitution_principle/#guidelines","text":"Ask yourself the following questions when implementing interfaces/base classes:\u200b Is every member/method suitable for each concrete implementation?\u200b Is the naming of the interface/base class open enough for later implementations?","title":"Guidelines"},{"location":"principles/open_closed_principle/","text":"Definition A class is closed, since it may be compiled, stored in a library, baselined, and used by client classes. But it is also open, since any new class may use it as parent, adding new features. When a descendant class is defined, there is no need to change the original or to disturb its clients. Software entities [..] should be open for extension, but closed for modification. - Betrand Meyer Guidelines Keep switch cases and \u201eif else if\u201c chains at a bare minimum\u200b Use pattern, e.g. Strategy, to avoid inheritance \u200b Try to anticipate future extensions, but keep KISS and YAGNI in mind\u200b Don't overestimate your ability to anticipate \uf04a\u200b","title":"Open-closed principle"},{"location":"principles/open_closed_principle/#definition","text":"A class is closed, since it may be compiled, stored in a library, baselined, and used by client classes. But it is also open, since any new class may use it as parent, adding new features. When a descendant class is defined, there is no need to change the original or to disturb its clients. Software entities [..] should be open for extension, but closed for modification. - Betrand Meyer","title":"Definition"},{"location":"principles/open_closed_principle/#guidelines","text":"Keep switch cases and \u201eif else if\u201c chains at a bare minimum\u200b Use pattern, e.g. Strategy, to avoid inheritance \u200b Try to anticipate future extensions, but keep KISS and YAGNI in mind\u200b Don't overestimate your ability to anticipate \uf04a\u200b","title":"Guidelines"},{"location":"principles/single_responsibility_principle/","text":"Definition The single responsibility principle is a computer programming principle that states that every module, class, or function should have responsibility over a single part of the functionality provided by the software, and that responsibility should be entirely encapsulated by the class. A class should have only one reason to change [..]. - Robert C. Martin Guidelines Isolate cross cutting concerns and domain specific classes\u200b Dont reference external frameworks in POCO/DTO\u200b Prevent god classes to increase cohesion and readability\u200b Give meaningful names\u200b Write small classes so that everything can be seen at a glance\u200b (10 seconds) Cohesion Cohesion refers to the degree to which the elements inside a module belong together\u200b Measure of the strength of relationship between the class's methods and data themselves\u200b Modules can be scoped as applications, projects, classes or methods\u200b Cohesion is balanced with both unit complexity and coupling\u200b High cohesion often correlates with loose coupling\u200b","title":"Single Responsibility principle"},{"location":"principles/single_responsibility_principle/#definition","text":"The single responsibility principle is a computer programming principle that states that every module, class, or function should have responsibility over a single part of the functionality provided by the software, and that responsibility should be entirely encapsulated by the class. A class should have only one reason to change [..]. - Robert C. Martin","title":"Definition"},{"location":"principles/single_responsibility_principle/#guidelines","text":"Isolate cross cutting concerns and domain specific classes\u200b Dont reference external frameworks in POCO/DTO\u200b Prevent god classes to increase cohesion and readability\u200b Give meaningful names\u200b Write small classes so that everything can be seen at a glance\u200b (10 seconds)","title":"Guidelines"},{"location":"principles/single_responsibility_principle/#cohesion","text":"Cohesion refers to the degree to which the elements inside a module belong together\u200b Measure of the strength of relationship between the class's methods and data themselves\u200b Modules can be scoped as applications, projects, classes or methods\u200b Cohesion is balanced with both unit complexity and coupling\u200b High cohesion often correlates with loose coupling\u200b","title":"Cohesion"},{"location":"requirements/domain_driven_design/","text":"Refactoring methods Refactoring Use case Eric Evan's bubbles Used to add requirements in a separate bounded context without having to modify the architecture. Extract bounded contexts Cutting out an identified bounded context from an existing architecture. Nick Tune's toolbox Holistic iterative approach, with which a complete roadmap including evaluation of refactorings can be created. Glossary Phrase Meaning Anti Corruption Layer (ACL) Abstraction, to distinguish a bounded context from legacy. Here, the data should not simply be passed through, but transferred into models that make sense in the context. Bounded Context Global processes are cut in Bounded Context. Two questions to define the boundaries of the context: Is a requirement in the same Ubiquitous Language? Does this increase the complexity excessively? Ubiquitous Language Language spoken by experts in the domain/subdomain. No translations are enforced! Further reads https://github.com/ddd-crew","title":"Domain driven design"},{"location":"requirements/domain_driven_design/#refactoring-methods","text":"Refactoring Use case Eric Evan's bubbles Used to add requirements in a separate bounded context without having to modify the architecture. Extract bounded contexts Cutting out an identified bounded context from an existing architecture. Nick Tune's toolbox Holistic iterative approach, with which a complete roadmap including evaluation of refactorings can be created.","title":"Refactoring methods"},{"location":"requirements/domain_driven_design/#glossary","text":"Phrase Meaning Anti Corruption Layer (ACL) Abstraction, to distinguish a bounded context from legacy. Here, the data should not simply be passed through, but transferred into models that make sense in the context. Bounded Context Global processes are cut in Bounded Context. Two questions to define the boundaries of the context: Is a requirement in the same Ubiquitous Language? Does this increase the complexity excessively? Ubiquitous Language Language spoken by experts in the domain/subdomain. No translations are enforced!","title":"Glossary"},{"location":"requirements/domain_driven_design/#further-reads","text":"https://github.com/ddd-crew","title":"Further reads"},{"location":"requirements/domain_driven_design/eric_evans_bubbles/","text":"","title":"Eric evans bubbles"},{"location":"requirements/domain_driven_design/extract_bounded_contexts/","text":"","title":"Extract bounded contexts"},{"location":"requirements/domain_driven_design/nick_tunes_toolbox/","text":"","title":"Nick tunes toolbox"},{"location":"tips_and_more/","text":"dotfiles aserowy/dots : dotfiles working in linux and windows (in cygwin) nvim configurations aserowy/neocode : configuration to work fluent in nvim and vscode","title":"Tips and more"},{"location":"tips_and_more/#dotfiles","text":"aserowy/dots : dotfiles working in linux and windows (in cygwin)","title":"dotfiles"},{"location":"tips_and_more/#nvim-configurations","text":"aserowy/neocode : configuration to work fluent in nvim and vscode","title":"nvim configurations"},{"location":"tips_and_more/csharp/","text":"Async/await https://devblogs.microsoft.com/dotnet/configureawait-faq/ Code style (per project) https://www.nuget.org/packages/Microsoft.CodeAnalysis.FxCopAnalyzers/ https://docs.microsoft.com/de-de/visualstudio/code-quality/install-fxcop-analyzers?view=vs-2019 Profiling https://medium.com/@maxnalsky/optimizing-garbage-collection-in-a-high-load-net-web-service-3bb620b444a7 Testing Emailing in dev with smtp4dev Mocking ILogger<> Using custom LoggerAdapter as abstraction Adapter calls extension methods of ILogger Concrete implementation which can be mocked by e.g. moq","title":"C#"},{"location":"tips_and_more/csharp/#asyncawait","text":"https://devblogs.microsoft.com/dotnet/configureawait-faq/","title":"Async/await"},{"location":"tips_and_more/csharp/#code-style-per-project","text":"https://www.nuget.org/packages/Microsoft.CodeAnalysis.FxCopAnalyzers/ https://docs.microsoft.com/de-de/visualstudio/code-quality/install-fxcop-analyzers?view=vs-2019","title":"Code style (per project)"},{"location":"tips_and_more/csharp/#profiling","text":"https://medium.com/@maxnalsky/optimizing-garbage-collection-in-a-high-load-net-web-service-3bb620b444a7","title":"Profiling"},{"location":"tips_and_more/csharp/#testing","text":"Emailing in dev with smtp4dev Mocking ILogger<> Using custom LoggerAdapter as abstraction Adapter calls extension methods of ILogger Concrete implementation which can be mocked by e.g. moq","title":"Testing"},{"location":"tips_and_more/git/","text":".gitignore Generator for .gitignore Strategies Release flow used by MS. (trunk based with feature branches)","title":"Git"},{"location":"tips_and_more/git/#gitignore","text":"Generator for .gitignore","title":".gitignore"},{"location":"tips_and_more/git/#strategies","text":"Release flow used by MS. (trunk based with feature branches)","title":"Strategies"},{"location":"tips_and_more/perf/","text":"Benchmarks https://benchmarksgame-team.pages.debian.net/benchmarksgame/ https://www.techempower.com/benchmarks/","title":"#perf matters"},{"location":"tips_and_more/perf/#benchmarks","text":"https://benchmarksgame-team.pages.debian.net/benchmarksgame/ https://www.techempower.com/benchmarks/","title":"Benchmarks"},{"location":"tips_and_more/rust/","text":"Common cheat sheet - THE cheat sheet! Testing Rust Testability - a good overview of sources regarding tests","title":"Rust"},{"location":"tips_and_more/rust/#common","text":"cheat sheet - THE cheat sheet!","title":"Common"},{"location":"tips_and_more/rust/#testing","text":"Rust Testability - a good overview of sources regarding tests","title":"Testing"},{"location":"tips_and_more/visual_studio/","text":"Code cleanup With the introduction of .editorconfig you can introduce style profiles on a solution basis. Thus, every developer uses the same formatting. .editorconfig is inheritable. You can specify a root config on solution level and add specific customizations on project level. https://docs.microsoft.com/en-us/visualstudio/ide/code-styles-and-code-cleanup https://marketplace.visualstudio.com/items?itemName=MadsKristensen.CodeCleanupOnSave Commands Command Scheme Binding Edit.GoToAll Visual Studio 6 CTRL + , Configurations Options > Environment > Tabs and Windows > \"Pinned Tabs: Show pinned tabs in separate row\" Options > Projects and Solutions > General > \"Track Active Item in Solution Explorer\" Features Adding naming conventions for private fields with underscore Options > Text Editor > C# > Code Style > Naming Manage naming styles > Add Name: \"Begins with _\" Required Prefix: \"_\" Capitalization: camel Case Name Add Specification: Private or Internal Field Required Style: Begins with _","title":"Visual Studio"},{"location":"tips_and_more/visual_studio/#code-cleanup","text":"With the introduction of .editorconfig you can introduce style profiles on a solution basis. Thus, every developer uses the same formatting. .editorconfig is inheritable. You can specify a root config on solution level and add specific customizations on project level. https://docs.microsoft.com/en-us/visualstudio/ide/code-styles-and-code-cleanup https://marketplace.visualstudio.com/items?itemName=MadsKristensen.CodeCleanupOnSave","title":"Code cleanup"},{"location":"tips_and_more/visual_studio/#commands","text":"Command Scheme Binding Edit.GoToAll Visual Studio 6 CTRL + ,","title":"Commands"},{"location":"tips_and_more/visual_studio/#configurations","text":"Options > Environment > Tabs and Windows > \"Pinned Tabs: Show pinned tabs in separate row\" Options > Projects and Solutions > General > \"Track Active Item in Solution Explorer\"","title":"Configurations"},{"location":"tips_and_more/visual_studio/#features","text":"","title":"Features"},{"location":"tips_and_more/visual_studio/#adding-naming-conventions-for-private-fields-with-underscore","text":"Options > Text Editor > C# > Code Style > Naming Manage naming styles > Add Name: \"Begins with _\" Required Prefix: \"_\" Capitalization: camel Case Name Add Specification: Private or Internal Field Required Style: Begins with _","title":"Adding naming conventions for private fields with underscore"},{"location":"tips_and_more/go/","text":"Links Inlining static files","title":"Go"},{"location":"tips_and_more/go/#links","text":"Inlining static files","title":"Links"},{"location":"tips_and_more/go/array_operations/","text":"AppendVector a = append ( a , b ... ) Copy b = make ([] T , len ( a )) copy ( b , a ) // or b = append ([] T ( nil ), a ... ) Cut a = append ( a [: i ], a [ j :] ... ) To ensure gc of structs with pointer fields or pointer as elements and prevent possible memory leaks use the following. copy ( a [ i :], a [ j :]) for k , n := len ( a ) - j + i , len ( a ); k < n ; k ++ { a [ k ] = nil // or the zero value of T } a = a [: len ( a ) - j + i ] Delete a = append ( a [: i ], a [ i + 1 :] ... ) // or a = a [: i + copy ( a [ i :], a [ i + 1 :])] To ensure gc of structs with pointer fields or pointer as elements and prevent possible memory leaks use the following. copy ( a [ i :], a [ i + 1 :]) a [ len ( a ) - 1 ] = nil // or the zero value of T a = a [: len ( a ) - 1 ] Delete without preserving order a [ i ] = a [ len ( a ) - 1 ] a = a [: len ( a ) - 1 ] To ensure gc of structs with pointer fields or pointer as elements and prevent possible memory leaks use the following. a [ i ] = a [ len ( a ) - 1 ] a [ len ( a ) - 1 ] = nil a = a [: len ( a ) - 1 ] Expand a = append ( a [: i ], append ( make ([] T , j ), a [ i :] ... ) ... ) Extend a = append ( a , make ([] T , j ) ... ) Insert a = append ( a [: i ], append ([] T { x }, a [ i :] ... ) ... ) The second append creates a new slice with its own underlying storage and copies elements in a[i:] to that slice, and these elements are then copied back to slice a (by the first append). The creation of the new slice (and thus memory garbage) and the second copy can be avoided by using an alternative way: s = append ( s , 0 ) copy ( s [ i + 1 :], s [ i :]) s [ i ] = x InsertVector a = append ( a [: i ], append ( b , a [ i :] ... ) ... ) Pop x , a = a [ 0 ], a [ 1 :] Pop Back x , a = a [ len ( a ) - 1 ], a [: len ( a ) - 1 ] Push a = append ( a , x ) PushFront a = append ([] T { x }, a ... ) Shift x , a := a [ 0 ], a [ 1 :] Unshift a = append ([] T { x }, a ... )","title":"Array Operations"},{"location":"tips_and_more/go/array_operations/#appendvector","text":"a = append ( a , b ... )","title":"AppendVector"},{"location":"tips_and_more/go/array_operations/#copy","text":"b = make ([] T , len ( a )) copy ( b , a ) // or b = append ([] T ( nil ), a ... )","title":"Copy"},{"location":"tips_and_more/go/array_operations/#cut","text":"a = append ( a [: i ], a [ j :] ... ) To ensure gc of structs with pointer fields or pointer as elements and prevent possible memory leaks use the following. copy ( a [ i :], a [ j :]) for k , n := len ( a ) - j + i , len ( a ); k < n ; k ++ { a [ k ] = nil // or the zero value of T } a = a [: len ( a ) - j + i ]","title":"Cut"},{"location":"tips_and_more/go/array_operations/#delete","text":"a = append ( a [: i ], a [ i + 1 :] ... ) // or a = a [: i + copy ( a [ i :], a [ i + 1 :])] To ensure gc of structs with pointer fields or pointer as elements and prevent possible memory leaks use the following. copy ( a [ i :], a [ i + 1 :]) a [ len ( a ) - 1 ] = nil // or the zero value of T a = a [: len ( a ) - 1 ]","title":"Delete"},{"location":"tips_and_more/go/array_operations/#delete-without-preserving-order","text":"a [ i ] = a [ len ( a ) - 1 ] a = a [: len ( a ) - 1 ] To ensure gc of structs with pointer fields or pointer as elements and prevent possible memory leaks use the following. a [ i ] = a [ len ( a ) - 1 ] a [ len ( a ) - 1 ] = nil a = a [: len ( a ) - 1 ]","title":"Delete without preserving order"},{"location":"tips_and_more/go/array_operations/#expand","text":"a = append ( a [: i ], append ( make ([] T , j ), a [ i :] ... ) ... )","title":"Expand"},{"location":"tips_and_more/go/array_operations/#extend","text":"a = append ( a , make ([] T , j ) ... )","title":"Extend"},{"location":"tips_and_more/go/array_operations/#insert","text":"a = append ( a [: i ], append ([] T { x }, a [ i :] ... ) ... ) The second append creates a new slice with its own underlying storage and copies elements in a[i:] to that slice, and these elements are then copied back to slice a (by the first append). The creation of the new slice (and thus memory garbage) and the second copy can be avoided by using an alternative way: s = append ( s , 0 ) copy ( s [ i + 1 :], s [ i :]) s [ i ] = x","title":"Insert"},{"location":"tips_and_more/go/array_operations/#insertvector","text":"a = append ( a [: i ], append ( b , a [ i :] ... ) ... )","title":"InsertVector"},{"location":"tips_and_more/go/array_operations/#pop","text":"x , a = a [ 0 ], a [ 1 :]","title":"Pop"},{"location":"tips_and_more/go/array_operations/#pop-back","text":"x , a = a [ len ( a ) - 1 ], a [: len ( a ) - 1 ]","title":"Pop Back"},{"location":"tips_and_more/go/array_operations/#push","text":"a = append ( a , x )","title":"Push"},{"location":"tips_and_more/go/array_operations/#pushfront","text":"a = append ([] T { x }, a ... )","title":"PushFront"},{"location":"tips_and_more/go/array_operations/#shift","text":"x , a := a [ 0 ], a [ 1 :]","title":"Shift"},{"location":"tips_and_more/go/array_operations/#unshift","text":"a = append ([] T { x }, a ... )","title":"Unshift"},{"location":"tips_and_more/go/array_tricks/","text":"Filtering without allocating This trick uses the fact that a slice shares the same backing array and capacity as the original, so the storage is reused for the filtered slice. Of course, the original contents are modified. b := a [: 0 ] for _ , x := range a { if f ( x ) { b = append ( b , x ) } } Reversing To replace the contents of a slice with the same elements but in reverse order: for i := len ( a ) / 2 - 1 ; i >= 0 ; i -- { opp := len ( a ) - 1 - i a [ i ], a [ opp ] = a [ opp ], a [ i ] } The same thing, except with two indices: for left , right := 0 , len ( a ) - 1 ; left < right ; left , right = left + 1 , right - 1 { a [ left ], a [ right ] = a [ right ], a [ left ] } Shuffling Implementation of Fisher\u2013Yates algorithm: for i := len ( a ) - 1 ; i > 0 ; i -- { j := rand . Intn ( i + 1 ) a [ i ], a [ j ] = a [ j ], a [ i ] }","title":"Array tricks"},{"location":"tips_and_more/go/array_tricks/#filtering-without-allocating","text":"This trick uses the fact that a slice shares the same backing array and capacity as the original, so the storage is reused for the filtered slice. Of course, the original contents are modified. b := a [: 0 ] for _ , x := range a { if f ( x ) { b = append ( b , x ) } }","title":"Filtering without allocating"},{"location":"tips_and_more/go/array_tricks/#reversing","text":"To replace the contents of a slice with the same elements but in reverse order: for i := len ( a ) / 2 - 1 ; i >= 0 ; i -- { opp := len ( a ) - 1 - i a [ i ], a [ opp ] = a [ opp ], a [ i ] } The same thing, except with two indices: for left , right := 0 , len ( a ) - 1 ; left < right ; left , right = left + 1 , right - 1 { a [ left ], a [ right ] = a [ right ], a [ left ] }","title":"Reversing"},{"location":"tips_and_more/go/array_tricks/#shuffling","text":"Implementation of Fisher\u2013Yates algorithm: for i := len ( a ) - 1 ; i > 0 ; i -- { j := rand . Intn ( i + 1 ) a [ i ], a [ j ] = a [ j ], a [ i ] }","title":"Shuffling"}]}